{
    "dataset_param": {
        "dataset_type": "lumina",
        "basic_parameters": {
            "data_config": {
                "path": "/path/data.json"
            }
        },
        "preprocess_parameters": {
            "item_processor_type": "TokenizerItemProcessor",
            "patch_size": 32,
            "target_size": 256
        },
        "tokenizer_config": {
            "hub_backend": "hf",
            "autotokenizer_name": "luminaMGPT2Tokenizer",
            "from_pretrained": "./weights/Lumina/Lumina-mGPT-2.0/"
        }
    },
    "dataloader_param": {
        "dataloader_mode": "sampler",
        "sampler_type": "LuminaMetaLenDistSampler",
        "shuffle": false,
        "drop_last": true,
        "pin_memory": true,
        "group_frame": false,
        "group_resolution": false,
        "collate_param": {}
    }
}