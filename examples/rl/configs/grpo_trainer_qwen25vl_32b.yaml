model: /path/MindSpeed-MM/configs/model/qwen2.5vl_32b.json # 模型配置文件的绝对路径

megatron_training:
  model: qwen2.5vl_32b
  use_fused_rmsnorm: true
  normalization: RMSNorm
  use_mcore_models: true
  sequence_parallel: false
  use_flash_attn: true
  no_masked_softmax_fusion: true
  attention_softmax_in_fp32: true
  no_gradient_accumulation_fusion: true
  use_fused_swiglu: true
  swiglu: true
  use_fused_rotary_pos_emb: true
  position_embedding_type: rope
  bf16: true
  use_distributed_optimizer: true
  tokenizer_type: PretrainedFromHF
  tokenizer_name_or_path: /path/Qwen2.5-VL-32B-Instruct # 原始权重绝对路径
  global_batch_size: 256
  seq_length: 1024
  save_interval: 5000
  train_iters: 1000
  stage: ray_grpo
  attention_dropout: 0.0
  init_method_std: 0.01
  hidden_dropout: 0.0
  distributed_backend: nccl
  no_shared_storage: true
  variable_seq_lengths: true # 动态PP
  dataset_additional_keys: ['input_ids', 'input_ids_length', 'attention_mask', 'position_ids']
  data_path: /path/dataset/rl_data
  split: 100,0,0
  no_shuffle: true
  seed: 1234
  use_deter_comp: false # 是否开启确定性计算

actor_config:
  model: qwen2.5vl_32b
  micro_batch_size: 1
  tensor_model_parallel_size: 8
  pipeline_model_parallel_size: 2
  lr: 1e-6
  lr_decay_style: constant
  min_lr: 0.0
  weight_decay: 0.01
  lr_warmup_fraction: 0.0
  clip_grad: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  finetune: true
  load: /path/qwen2_5_vl_32b_tp8pp2 # megatron格式的权重路径
  save: ./ckpt
  no_load_optim: true
  no_load_rng: true
  num_workers: 8

vit_config:
  model: qwen2.5vl_32b
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  micro_batch_size: 16
  load: /path/qwen2_5_vl_32b_vit_tp1pp1 # 视觉模型megatron格式的权重路径
  no_load_optim: true
  no_load_rng: true

rl_config:
  # 环境变量地址（相对路径）
  runtime_env_path: MindSpeed-MM/examples/rl/envs/runtime_env.yaml
  is_multimodal: true
  guarantee_order: true # TD 是否保序
  use_integrated_worker: true # 是否共卡
  blocking: true
  gamma: 1.0
  lam: 0.95
  use_dynamic_bsz: true
  max_packing_token_size: [90000, 4000]
  actor_forward_micro_batch_size: 1
  ref_forward_micro_batch_size: 1
  reward_dispatch_size: 5
  adv_dispatch_size: 5
  adv_estimator: group_norm
  kl_penalty: low_var_kl
  kl_ctrl_type: fixed
  init_kl_coef: 0.01
  mini_batch_size: 32
  max_prompt_length: 1024
  epochs: 1
  clip_ratio: 0.2
  entropy_coeff: 0.0
  shuffle_mini_batch: false
  n_samples_per_prompt: 5
  rule_reward: true
  verifier_function: ["base_acc"]
  verifier_weight: [1.0]
  num_cpus_for_local_task: 1.0
  use_tensorboard: false
  reuse_image_embeds: true
  colocate_actor_and_vit: true
  actor_resource:
    num_npus: 16
  vit_resource:
    num_npus: 16

generate_config:
  enforce_eager: true
  trust_remote_code: true
  offload_train_optimizer: true
  offload_train_grad: true
  offload_train_param: true

  # 推理时的并行配置
  infer_tensor_parallel_size: 2
  infer_pipeline_parallel_size: 1
  infer_expert_parallel_size: 1

  # vllm 模型相关设置
  max_num_seqs: 32
  max_model_len: 3072 # prompt_length + response_length
  dtype: "bfloat16"
  gpu_memory_utilization: 0.9

  # 采样配置
  sampling_config:
    logprobs: 0
    max_tokens: 2048
    top_p: 1.0
    top_k: -1
    min_p: 0.0
    temperature: 1.0
    detokenize: false
    seed: 1234
