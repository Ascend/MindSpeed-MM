{
    "model_id": "qwen2vl",
    "ps_version": "v2",
    "params_dtype": "bf16",
    "image_encoder": {
        "vision_encoder": {
            "model_id": "qwen2vit",
            "window_size": null,
            "num_layers": 2,
            "hidden_size": 1280,
            "ffn_hidden_size": 5120,
            "llm_hidden_size": 3584,
            "num_attention_heads": 16,
            "hidden_dropout": 0.0,
            "attention_dropout": 0.0,
            "in_channels": 3,
            "patch_size": 14,
            "spatial_merge_size": 2,
            "spatial_patch_size": 14,
            "temporal_patch_size": 2,
            "layernorm_epsilon": 1e-06,
            "normalization": "LayerNorm",
            "fp16": false,
            "bf16": true,
            "params_dtype": "bf16",
            "apply_rope_fusion": true,
            "activation_func": "quick_gelu",
            "freeze": true,
            "ckpt_path": "/<your_clip_weights_path>/converted_clip.pt"
        },
        "vision_projector": {
            "model_id": "lnmlp",
            "num_layers": 1,
            "num_attention_heads": 1,
            "gated_linear_unit": false,
            "bias_activation_fusion": false,
            "add_bias_linear": true,
            "input_size": 1280,
            "hidden_size": 3584,
            "ffn_hidden_size": 5120,
            "activation_func": "gelu",
            "bf16": true,
            "params_dtype": "bf16",
            "freeze": true,
            "ckpt_path": null
        }
    },
    "text_decoder": {
        "num_layer_list": [0, 1],
        "padded_vocab_size": 152064,
        "parallel_output": true,
        "rope_theta": 1000000.0,
        "position_embedding_type": "rope",
        "num_layers": 2,
        "hidden_size": 3584,
        "gated_linear_unit": true,
        "ffn_hidden_size": 18944,
        "num_attention_heads": 28,
        "max_position_embeddings": 32768,
        "attention_dropout": 0.0,
        "init_method_std": 0.01,
        "hidden_dropout": 0.0,
        "normalization": "RMSNorm",
        "layernorm_epsilon": 1e-6,
        "add_bias_linear": false,
        "add_qkv_bias": true,
        "bias_activation_fusion": true,
        "bias_dropout_fusion": true,
        "persist_layer_norm": true,
        "apply_rope_fusion": true,
        "async_tensor_model_parallel_allreduce": true,
        "activation_func": "silu",
        "attention_softmax_in_fp32": true,
        "params_dtype": "bf16",
        "bf16": true,
        "num_query_groups": 4
    },
    "text_encoder": null,
    "video_encoder": null
}