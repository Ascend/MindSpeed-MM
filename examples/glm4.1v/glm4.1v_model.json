{
    "model_id": "glm4.1v",
    "img_context_token_id": 151343,
    "video_token_id": 151344,
    "vision_start_token_id": 151341,
    "image_encoder": {
        "vision_encoder": {
            "model_id": "glm4v_vit",
            "num_layers": 24,
            "pipeline_num_layers": [24,0,0,0],
            "hidden_size": 1536,
            "attention_bias": false,
            "context_parallel_size": 2,
            "out_hidden_size": 4096,
            "ffn_hidden_size": 4096,
            "llm_hidden_size": 1280,
            "gated_linear_unit": true,
            "add_bias_linear": false,
            "num_attention_heads": 12,
            "hidden_dropout": 0.0,
            "attention_dropout": 0.0,
            "in_channels": 3,
            "patch_size": 14,
            "spatial_merge_size": 2,
            "temporal_patch_size": 2,
            "layernorm_epsilon": 1e-06,
            "normalization": "RMSNorm",
            "image_size": 336,
            "fp16": false,
            "bf16": true,
            "params_dtype": "bf16",
            "activation_func": "silu",
            "hidden_act": "silu",
            "freeze": true,
            "use_fused_rotary_pos_emb": true,
            "post_layer_norm": false,
            "tokens_per_second": 2,
            "rms_norm_eps": 1e-05,
            "window_attn_size": 112,
            "recompute_granularity": "full",
            "recompute_method": "uniform",
            "recompute_num_layers": 1
        },
        "vision_projector": {
            "model_id": "GlmMLP",
            "num_layers": 1,
            "gated_linear_unit": false,
            "bias_activation_fusion": false,
            "add_bias_linear": true,
            "input_size": 1280,
            "hidden_size": 2048,
            "ffn_hidden_size": 5120,
            "bf16": true,
            "params_dtype": "bf16",
            "freeze": true,
            "layernorm_epsilon": 1e-06,
            "normalization": "RMSNorm",
            "out_hidden_size": 4096,
            "intermediate_size": 13696,
            "hidden_act": "silu"
        }
    },
    "text_decoder": {
        "model_id": "glm4v_lm",
        "num_layers": 40,
        "pipeline_num_layers": [7, 11, 11, 11],
        "hidden_size": 4096,
        "ffn_hidden_size": 13696,
        "num_attention_heads": 32,
        "context_parallel_size": 2,
        "max_position_embeddings": 128000,
        "vocab_size": 151552,
        "rope_theta": 10000.0,
        "untie_embeddings_and_output_weights": true,
        "disable_bias_linear": true,
        "attention_dropout": 0.0,
        "init_method_std": 0.01,
        "hidden_dropout": 0.0,
        "position_embedding_type": "mrope",
        "normalization": "RMSNorm",
        "activation_func": "silu",
        "use_fused_rotary_pos_emb": true,
        "partial_rotary_factor": 0.5,
        "attention_softmax_in_fp32": true,
        "params_dtype": "bf16",
        "hidden_act": "silu",
        "bf16": true,
        "parallel_output": true,
        "group_query_attention": true,
        "num_query_groups": 2,
        "num_key_value_heads": 2,
        "mrope_section": [8, 12, 12],
        "rope_scaling": null,
        "gated_linear_unit": true,
        "layernorm_epsilon": 1e-06,
        "add_bias_linear":false,
        "add_qkv_bias": true,
        "use_remove_padding": false,
        "rms_norm_eps": 1e-05

    },
    "text_encoder": null,
    "video_encoder": null
}
